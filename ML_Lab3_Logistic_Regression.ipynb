{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnV/vwIsU6P34bREvi2uLf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayaarosama/Machine-Learning-fundamentals/blob/main/ML_Lab3_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "MqtwR6dQ49zt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes is a simple yet powerful classification algorithm based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Here, we will consider a Gaussian Naive Bayes classifier, which is suitable for continuous data.\n",
        "\n",
        "\n",
        "### Steps to Implement Gaussian Naive Bayes Classifier\n",
        "\n",
        "1. **Calculate Prior Probabilities**:\n",
        "   The prior probability of each class is the proportion of samples belonging to that class.\n",
        "\n",
        "2. **Calculate Mean and Variance**:\n",
        "   For each class, calculate the mean and variance of the feature values.\n",
        "\n",
        "3. **Calculate Likelihood**:\n",
        "   Using the Gaussian (Normal) distribution, calculate the likelihood of a feature value given a class.\n",
        "\n",
        "4. **Calculate Posterior Probabilities**:\n",
        "   Using Bayes' theorem, calculate the posterior probability for each class and choose the class with the highest posterior probability.\n"
      ],
      "metadata": {
        "id": "XSnTV7phMpCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step-by-Step Calculation\n",
        "\n",
        "1. **Prior Probabilities**:\n",
        "   $\n",
        "   P(y=0) = \\frac{4}{10} = 0.4, \\quad P(y=1) = \\frac{6}{10} = 0.6\n",
        "   $\n",
        "\n",
        "2. **Calculate Mean and Variance**:\n",
        "   - For class 0 (\\(y=0\\)):\n",
        "     $\n",
        "     \\text{mean}_0 = \\frac{1 + 2 + 3 + 4}{4} = 2.5, \\quad \\text{variance}_0 = \\frac{(1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2}{4} = 1.25\n",
        "     $\n",
        "   - For class 1 (\\(y=1\\)):\n",
        "     $\n",
        "     \\text{mean}_1 = \\frac{5 + 6 + 7 + 8 + 9 + 10}{6} = 7.5, \\quad \\text{variance}_1 = \\frac{(5-7.5)^2 + (6-7.5)^2 + (7-7.5)^2 + (8-7.5)^2 + (9-7.5)^2 + (10-7.5)^2}{6} = 2.9167\n",
        "     $\n",
        "\n",
        "3. **Calculate Likelihood**:\n",
        "   The likelihood for a Gaussian distribution is given by:\n",
        "   $\n",
        "   P(x|y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n",
        "   $\n",
        "   For a given feature value \\(x\\).\n",
        "\n",
        "4. **Posterior Probability**:\n",
        "   Using Bayes' theorem:\n",
        "   $\n",
        "   P(y|x) \\propto P(x|y) P(y)\n",
        "   $\n"
      ],
      "metadata": {
        "id": "BIkXdiz0NN5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Example Calculation for \\(x=3\\)\n",
        "\n",
        "1. **Likelihood for $y=0$**:\n",
        "\n",
        "   $\n",
        "   P(x=3|y=0) = \\frac{1}{\\sqrt{2\\pi \\cdot 1.25}} \\exp\\left(-\\frac{(3-2.5)^2}{2 \\cdot 1.25}\\right) = \\frac{1}{\\sqrt{2\\pi \\cdot 1.25}} \\exp\\left(-\\frac{0.5^2}{2 \\cdot 1.25}\\right)\n",
        "   $\n",
        "   \n",
        "2. **Likelihood for $y=1$**:\n",
        "\n",
        "   $\n",
        "   P(x=3|y=1) = \\frac{1}{\\sqrt{2\\pi \\cdot 2.9167}} \\exp\\left(-\\frac{(3-7.5)^2}{2 \\cdot 2.9167}\\right) = \\frac{1}{\\sqrt{2\\pi \\cdot 2.9167}} \\exp\\left(-\\frac{4.5^2}{2 \\cdot 2.9167}\\right)\n",
        "   $\n",
        "\n",
        "3. **Posterior Probability**:\n",
        "\n",
        "   $\n",
        "   P(y=0|x=3) \\propto P(x=3|y=0) \\cdot P(y=0)\n",
        "   $\n",
        "\n",
        "   $\n",
        "   P(y=1|x=3) \\propto P(x=3|y=1) \\cdot P(y=1)\n",
        "   $\n",
        "\n",
        "4. **Comparison**:\n",
        "\n",
        "   Compare $P(y=0|x=3)$ and $P(y=1|x=3)$ and choose the class with the higher probability."
      ],
      "metadata": {
        "id": "uKrcl2JCNUwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Data\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
        "\n",
        "# Create Gaussian Naive Bayes model\n",
        "model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict class for a new data point\n",
        "x_new = np.array([[5]])\n",
        "predicted_class = model.predict(x_new)\n",
        "predicted_proba = model.predict_proba(x_new)\n",
        "\n",
        "print(f\"Predicted class for x={x_new[0][0]}: {predicted_class[0]}\")\n",
        "print(f\"Class probabilities: {predicted_proba}\")\n"
      ],
      "metadata": {
        "id": "pPU8VC9rI1rb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e75768dc-8000-4864-f280-82a0ae058ae8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class for x=5: 1\n",
            "Class probabilities: [[0.19617293 0.80382707]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regresion"
      ],
      "metadata": {
        "id": "muTaStR85BSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression uses the logistic function, also known as the sigmoid function, to map predicted values to probabilities. The sigmoid function is defined as:\n",
        "\n",
        "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
        "\n"
      ],
      "metadata": {
        "id": "L5rfNJ6uO9fL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1Kp-1KaQIvDU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, $z$ is a linear combination of the input features. For a single feature $x $, it can be represented as:\n",
        "\n",
        "$ z = \\beta_0 + \\beta_1 x $\n"
      ],
      "metadata": {
        "id": "Qrr2OFFiSHGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where:\n",
        "- $ \\beta_0 $ is the intercept (bias term).\n",
        "- $ \\beta_1 $ is the coefficient for the feature $ x $."
      ],
      "metadata": {
        "id": "qpKqFD9VST7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic regression model then becomes:\n",
        "\n",
        "$ P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}} $\n",
        "\n",
        "Where $ P(y=1|x) $ is the probability that the output $ y $ is 1 given the input $x $.\n"
      ],
      "metadata": {
        "id": "5RJ76Q01ShPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_likelihood(X, y, beta):\n",
        "    z = np.dot(X, beta)\n",
        "    return np.sum(y * z - np.log(1 + np.exp(z)))\n"
      ],
      "metadata": {
        "id": "zS0GTfgLJmhc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(X, y, num_steps, learning_rate, add_intercept=True):\n",
        "    if add_intercept:\n",
        "        intercept = np.ones((X.shape[0], 1))\n",
        "        X = np.hstack((intercept, X))\n",
        "\n",
        "    beta = np.random.randn(X.shape[1])\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        z = np.dot(X, beta)\n",
        "        predictions = sigmoid(z)\n",
        "\n",
        "        # Gradient of the log-likelihood\n",
        "        gradient = np.dot(X.T, y - predictions)\n",
        "\n",
        "        # Update the coefficients\n",
        "        beta += learning_rate * gradient\n",
        "\n",
        "        # Print the log-likelihood every 1000 steps\n",
        "        if step % 1000 == 0:\n",
        "            print(f'Log-Likelihood: {log_likelihood(X, y, beta)}')\n",
        "\n",
        "    return beta\n"
      ],
      "metadata": {
        "id": "V1fku09FJnyj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_prob(X, beta):\n",
        "    if X.shape[1] == beta.shape[0] - 1:\n",
        "        intercept = np.ones((X.shape[0], 1))\n",
        "        X = np.hstack((intercept, X))\n",
        "\n",
        "    return sigmoid(np.dot(X, beta))\n"
      ],
      "metadata": {
        "id": "55hF4MIPJpFM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
        "\n",
        "# Train the model\n",
        "beta = logistic_regression(X, y, num_steps=20000, learning_rate=0.0001)\n",
        "\n",
        "# Make a prediction\n",
        "new_data = np.array([[5]])\n",
        "prob = predict_prob(new_data, beta)\n",
        "print(f\"Probability of passing: {prob[0]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibW2i-zlJqvU",
        "outputId": "54d9c793-49f5-4ad4-ddd1-d725ce338b60"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log-Likelihood: -4.532013407722934\n",
            "Log-Likelihood: -3.5549502221395963\n",
            "Log-Likelihood: -3.444658630254134\n",
            "Log-Likelihood: -3.343378762698607\n",
            "Log-Likelihood: -3.249188735181419\n",
            "Log-Likelihood: -3.1614260398192426\n",
            "Log-Likelihood: -3.0794985521207074\n",
            "Log-Likelihood: -3.0028767276985175\n",
            "Log-Likelihood: -2.931086874726406\n",
            "Log-Likelihood: -2.8637050171277325\n",
            "Log-Likelihood: -2.800351352283475\n",
            "Log-Likelihood: -2.7406852836810884\n",
            "Log-Likelihood: -2.6844009939555677\n",
            "Log-Likelihood: -2.63122351583452\n",
            "Log-Likelihood: -2.580905255313262\n",
            "Log-Likelihood: -2.533222921286035\n",
            "Log-Likelihood: -2.4879748176681775\n",
            "Log-Likelihood: -2.444978456948125\n",
            "Log-Likelihood: -2.4040684575589104\n",
            "Log-Likelihood: -2.3650946910967825\n",
            "Probability of passing: 0.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Sklearn"
      ],
      "metadata": {
        "id": "SsrD1VakOoVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Data\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
        "\n",
        "# Model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Prediction\n",
        "new_data = np.array([[5]])\n",
        "prediction = model.predict_proba(new_data)\n",
        "print(f\"Probability of passing: {prediction[0][1]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJiNatjFJtjT",
        "outputId": "2f9fdfdd-13aa-4ca8-c18f-4effb2dc8b90"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of passing: 0.65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(new_data)\n",
        "\n",
        "prediction[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdhOVBKsIlIf",
        "outputId": "f1b93098-fbc4-4fe0-c4f3-120f18bfe550"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[The Titanic Kaggle Dataset](https://www.kaggle.com/competitions/titanic/data)"
      ],
      "metadata": {
        "id": "-UYf3fKcLO-S"
      }
    }
  ]
}